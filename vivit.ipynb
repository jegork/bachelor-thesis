{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a315e49c-fbb4-4811-9f38-743d5cb1aefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vitb16 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Inflate a DINO Image transformer model into a DINO Video transformer model that uses 32 frames\n",
    "!python3 vivit_transformers/convert_vit_to_vivit.py --vit_model_path facebook/dino-vitb16 --tubelet_n 2 --video_length 32 --output_path vivit_dino_32_untrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c08bacd-f218-486b-bc93-56d72f5a5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import VIVIT_UCF101\n",
    "from transformers import ViTFeatureExtractor, TrainingArguments, Trainer\n",
    "from vivit_transformers import ViViTForImageClassification\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def compute_metrics(data):\n",
    "    return {'accuracy': accuracy_score(data.label_ids, data.predictions.argmax(-1))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008ca067-896f-4b14-b5ad-a45bcb6bcf9b",
   "metadata": {},
   "source": [
    "# Train ViViT with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424598a0-989a-4fd7-b0e7-8e974ad49167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vivit_transformers import ViViTConfig\n",
    "config = ViViTConfig.from_pretrained(\"./vivit_dino_32_untrained\", num_labels=101)\n",
    "\n",
    "# ViViT uses the same feature extractor as ViT (cropping, norm and etc)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\n",
    "\n",
    "dataset_train = VIVIT_UCF101('UCF-101', 'ucfTrainTestlist', 1, True, feature_extractor=feature_extractor)\n",
    "dataset_test = VIVIT_UCF101('UCF-101', 'ucfTrainTestlist', 1, False, feature_extractor=feature_extractor)\n",
    "\n",
    "model = ViViTForImageClassification(config)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True \n",
    "\n",
    "\n",
    "BATCH_SIZE=6\n",
    "GRAD_ACC_STEPS=2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./vivit_dino_32frames_random',          # output directory\n",
    "    num_train_epochs=15,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n",
    "    warmup_steps=10000/GRAD_ACC_STEPS ,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_strategy='steps',\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    learning_rate=5e-4,\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=500,\n",
    "    label_smoothing_factor=0.15,\n",
    "    save_total_limit=1,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    dataloader_num_workers=16\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset_train,         # training dataset\n",
    "    eval_dataset=dataset_test,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda851e2-a7b6-4323-997b-6398ef41bb92",
   "metadata": {},
   "source": [
    "# Train ViViT with DINO weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0625de-4958-44e0-8267-26ee7f829af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\n",
    "\n",
    "dataset_train = VIVIT_UCF101('UCF-101', 'ucfTrainTestlist', 1, True, feature_extractor=feature_extractor)\n",
    "dataset_test = VIVIT_UCF101('UCF-101', 'ucfTrainTestlist', 1, False, feature_extractor=feature_extractor)\n",
    "\n",
    "model = ViViTForImageClassification.from_pretrained('./vivit_dino_32_untrained', num_labels=101)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True \n",
    "\n",
    "\n",
    "BATCH_SIZE=6\n",
    "GRAD_ACC_STEPS=2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./vivit_dino_32frames',          # output directory\n",
    "    num_train_epochs=10,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n",
    "    warmup_steps=10000/GRAD_ACC_STEPS ,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_strategy='steps',\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    learning_rate=5e-5,\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=500,\n",
    "    label_smoothing_factor=0.15,\n",
    "    save_total_limit=1,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    dataloader_num_workers=16\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset_train,         # training dataset\n",
    "    eval_dataset=dataset_test,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a18321-0188-4595-8fb4-014b87afa5de",
   "metadata": {},
   "source": [
    "# Train ViViT with DINO weights with 32 keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4bc614-eceb-490f-b789-6b209ec61df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\n",
    "\n",
    "train_kfs = joblib.load(\"keyframes/train_01_32frames.pkl\") \n",
    "test_kfs = joblib.load(\"keyframes/test_01_32frames.pkl\")\n",
    "\n",
    "dataset_train = VIVIT_UCF101('UCF-101', 'ucfTrainTestlist', 1, True, feature_extractor=feature_extractor, frame_sampler=train_kfs)\n",
    "dataset_test = VIVIT_UCF101('UCF-101', 'ucfTrainTestlist', 1, False, feature_extractor=feature_extractor, frame_sampler=test_kfs)\n",
    "\n",
    "model = ViViTForImageClassification.from_pretrained('./vivit_dino_32_untrained', num_labels=101)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True \n",
    "\n",
    "\n",
    "BATCH_SIZE=6\n",
    "GRAD_ACC_STEPS=2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./vivit_dino_32frames',          # output directory\n",
    "    num_train_epochs=10,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n",
    "    warmup_steps=10000/GRAD_ACC_STEPS ,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_strategy='steps',\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True, \n",
    "    dataloader_pin_memory=True,\n",
    "    learning_rate=5e-5,\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=500,\n",
    "    label_smoothing_factor=0.15,\n",
    "    save_total_limit=1,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    dataloader_num_workers=16\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset_train,         # training dataset\n",
    "    eval_dataset=dataset_test,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b086d-5a49-44d5-8b56-1c98f220aec2",
   "metadata": {},
   "source": [
    "# Train ViViT with DINO weights with 10 keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c03dfc-9bf6-4a82-bd46-11be23a619e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vitb16 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Inflate a DINO Image transformer model into a DINO Video transformer model that uses 10 frames\n",
    "!python3 vivit_transformers/convert_vit_to_vivit.py --vit_model_path facebook/dino-vitb16 --tubelet_n 2 --video_length 10 --output_path vivit_dino_10_untrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1e734-7437-411c-95b7-60a48587634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import VIVIT_UCF101\n",
    "from transformers import ViTFeatureExtractor, TrainingArguments, Trainer\n",
    "from vivit_transformers import ViViTForImageClassification\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\n",
    "\n",
    "train_kfs = joblib.load(\"keyframes/train_01_10frames.pkl\") \n",
    "test_kfs = joblib.load(\"keyframes/test_01_10frames.pkl\")\n",
    "\n",
    "dataset_train = VIVIT_UCF101('UCF-101', 'ucfTrainTestlist', 1, True, feature_extractor=feature_extractor, frame_sampler=train_kfs)\n",
    "dataset_test = VIVIT_UCF101('UCF-101', 'ucfTrainTestlist', 1, False, feature_extractor=feature_extractor, frame_sampler=test_kfs)\n",
    "\n",
    "dataset_train.n_frames = 10\n",
    "dataset_test.n_frames = 10\n",
    "\n",
    "model = ViViTForImageClassification.from_pretrained('./vivit_dino_10_untrained', num_labels=101)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True \n",
    "\n",
    "\n",
    "BATCH_SIZE=12\n",
    "GRAD_ACC_STEPS=1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./vivit_dino_10frames_kmeans',          # output directory\n",
    "    num_train_epochs=10,              # total # of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n",
    "    warmup_steps=10000/GRAD_ACC_STEPS ,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_strategy='steps',\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    learning_rate=5e-5,\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=500,\n",
    "    label_smoothing_factor=0.15,\n",
    "    save_total_limit=1,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    dataloader_num_workers=16\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset_train,         # training dataset\n",
    "    eval_dataset=dataset_test,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_torch",
   "language": "python",
   "name": "conda_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
